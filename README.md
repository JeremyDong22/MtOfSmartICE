# Meituan Merchant Backend Crawler

Automated daily crawler for Meituan merchant backend membership card transaction data.

## Features

- ðŸ”Œ **CDP-only connection** - Connects to existing Chrome, no browser launching
- ðŸª **Multi-store support** - Automatically discovers and processes all stores
- ðŸ“Š **Daily automation** - Crawls yesterday's data automatically
- ðŸ—„ï¸ **SQLite database** - Stores data with full store_id + store_name tracking
- ðŸ”„ **Extensible architecture** - Easy to add new crawlers for other reports
- ðŸ“ **Comprehensive logging** - Daily log files with detailed tracking
- âš¡ **Error handling** - Retry logic, popup dismissal, graceful failures

## Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Start Chrome with Remote Debugging
```bash
google-chrome --remote-debugging-port=9222 --user-data-dir=/tmp/chrome-profile
```

### 3. Login to Meituan
Open the Chrome browser and login at https://pos.meituan.com

### 4. Run the Crawler
```bash
# Crawl yesterday's data for all stores
python src/main.py

# Crawl specific date
python src/main.py --date 2025-12-13

# Crawl single store
python src/main.py --store 58188193
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         main.py                              â”‚
â”‚              (Daily Crawler Entry Point)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”œâ”€â–º CDPSession (browser/cdp_session.py)
               â”‚   â””â”€ Connect to Chrome via CDP
               â”‚
               â”œâ”€â–º StoreNavigator (browser/store_navigator.py)
               â”‚   â”œâ”€ Navigate to dashboard
               â”‚   â”œâ”€ Get all stores from dropdown
               â”‚   â””â”€ Switch between stores
               â”‚
               â”œâ”€â–º MembershipCrawler (crawlers/membership_crawler.py)
               â”‚   â”œâ”€ Inherits from BaseCrawler
               â”‚   â”œâ”€ Navigate to report page
               â”‚   â”œâ”€ Set date filter
               â”‚   â”œâ”€ Select card type filters
               â”‚   â”œâ”€ Extract summary data
               â”‚   â””â”€ Extract order details
               â”‚
               â””â”€â–º DatabaseManager (database/db_manager.py)
                   â”œâ”€ Save membership data
                   â”œâ”€ Log crawl status
                   â””â”€ Export reports
```

## Directory Structure

```
src/
â”œâ”€â”€ main.py                      # Daily crawler entry point
â”œâ”€â”€ config.py                    # Centralized configuration
â”œâ”€â”€ browser/
â”‚   â”œâ”€â”€ cdp_session.py           # CDP-only browser connection
â”‚   â””â”€â”€ store_navigator.py       # Store discovery & switching
â”œâ”€â”€ crawlers/
â”‚   â”œâ”€â”€ base_crawler.py          # Abstract base class
â”‚   â””â”€â”€ membership_crawler.py    # Membership card crawler
â””â”€â”€ utils/
    â”œâ”€â”€ date_utils.py            # Date handling utilities
    â””â”€â”€ selectors.py             # CSS selectors

database/
â””â”€â”€ db_manager.py                # Database operations

data/
â””â”€â”€ meituan.db                   # SQLite database

logs/
â””â”€â”€ crawler_YYYYMMDD.log         # Daily log files
```

## Usage

### Command-Line Arguments

```bash
python src/main.py [options]

Options:
  --cdp URL         CDP endpoint URL (default: http://localhost:9222)
  --date YYYY-MM-DD Target date (default: yesterday)
  --store XXXXXXXX  Crawl specific store only
  --force           Force re-crawl (ignore existing data)
```

### Examples

**Daily automated crawl**:
```bash
python src/main.py
```

**Crawl specific date**:
```bash
python src/main.py --date 2025-12-13
```

**Crawl single store**:
```bash
python src/main.py --store 58188193
```

**Force re-crawl**:
```bash
python src/main.py --force --date 2025-12-13
```

## Database Schema

### stores
Stores merchant/store information:
- `merchant_id` (PRIMARY KEY)
- `store_name`
- `org_code`

### membership_card_data
Daily summary data:
- `merchant_id` + `date` (UNIQUE)
- `cards_opened`
- `total_amount`

### card_details
Individual transaction details:
- Linked to `membership_card_data`
- Order number, time, status, amounts
- Phone number, card number

### crawl_log
Crawl operation tracking:
- `merchant_id` + `crawler_type` + `date` (UNIQUE)
- `status` (success/failed/partial)
- `records_count`
- `error_message`

## Viewing Results

### Query Database
```bash
sqlite3 data/meituan.db

SELECT s.store_name, m.date, m.cards_opened, m.total_amount
FROM membership_card_data m
JOIN stores s ON m.merchant_id = s.merchant_id
ORDER BY m.date DESC
LIMIT 20;
```

### Export to CSV
```python
from database.db_manager import DatabaseManager

db = DatabaseManager()
db.export_to_csv("reports/data.csv", start_date="2025-12-01")
```

### Check Logs
```bash
tail -f logs/crawler_$(date +%Y%m%d).log
```

## Scheduling (Cron)

Run automatically at 2 AM daily:

```bash
crontab -e

# Add this line
0 2 * * * cd /path/to/MtOfSmartICE && /path/to/venv/bin/python src/main.py >> logs/cron.log 2>&1
```

**Note**: Ensure Chrome is running with CDP before cron job executes.

## Adding New Crawlers

1. Create new crawler class inheriting from `BaseCrawler`:
```python
from src.crawlers.base_crawler import BaseCrawler

class NewCrawler(BaseCrawler):
    async def crawl(self, store_id: str, store_name: str) -> Dict[str, Any]:
        # Implement crawling logic
        return self.create_result(True, store_id, store_name, data=data)
```

2. Update `main.py` to use the new crawler

3. Add database tables if needed

See [ARCHITECTURE.md](ARCHITECTURE.md) for detailed guide.

## Documentation

- **[QUICKSTART.md](QUICKSTART.md)** - Quick start guide with examples
- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Detailed architecture documentation
- **[REFACTORING_SUMMARY.md](REFACTORING_SUMMARY.md)** - Refactoring details and migration guide

## Troubleshooting

**CDP connection failed**:
```bash
# Check if Chrome is running
lsof -i :9222

# Start Chrome with CDP
google-chrome --remote-debugging-port=9222 --user-data-dir=/tmp/chrome-profile
```

**No stores found**:
1. Open Chrome and navigate to https://pos.meituan.com/web/marketing/home
2. Verify store dropdown is visible in top-right
3. Re-run crawler

**Data already exists**:
```bash
# Use --force to re-crawl
python src/main.py --force
```

For more troubleshooting tips, see [QUICKSTART.md](QUICKSTART.md#troubleshooting).

## Requirements

- Python 3.8+
- Chrome/Chromium browser
- playwright
- sqlite3
- Valid Meituan merchant account

## License

Internal use only.

## Support

Check logs in `logs/` directory for detailed information about crawl operations.

---

**Version**: 2.0 (Refactored Architecture)
**Last Updated**: 2025-12-14
